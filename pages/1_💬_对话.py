from llama_index.core.embeddings import resolve_embed_model
from llama_index.llms.openai_like import OpenAILike
from llama_index.core import Settings
from llama_index.legacy.callbacks import CallbackManager

# for loading environment variables
from decouple import config
import pathlib

import streamlit as st

from storage_utils import loadindex

@st.cache_resource
def init_models():

    callback_manager = CallbackManager()

    # è¯»å–å‚æ•°
    api_key = config("API_KEY")
    base_url = config("BASE_URL")
    model_name = config("MODEL_NAME")

    # åŠ è½½embed_model
    print("Loading embed_model...")
    embed_model_name = "paraphrase-multilingual-MiniLM-L12-v2"
    embed_model_path = pathlib.Path(st.session_state["project_path"]).joinpath(embed_model_name)
    embed_model = resolve_embed_model("local:" + str(embed_model_path))
    print("embed_model_path:", embed_model_path)
    Settings.embed_model = embed_model
    print("embed_model loaded.")

    # åˆå§‹åŒ–llm
    print("Initializing llm...")
    llm = OpenAILike(
        api_base=base_url, api_key=api_key, model=model_name, is_chat_model=True, callback_manager=callback_manager
    )
    Settings.llm = llm
    print("llm initialized.")

    return


def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„åŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"}]


def greet2(question):
    rsp = st.session_state['query_engine'].query(question)
    return rsp


def generate_llama_index_response(prompt_input):
    return greet2(prompt_input)


if __name__ == "__main__":
    st.set_page_config(page_title="rag_demo", page_icon="ğŸ’¬")
    st.title("rag_demo")

    # åˆå§‹åŒ–æ¨¡å‹
    if 'init_models' not in st.session_state:
        init_models()
        st.session_state['init_models'] = True

    # æ£€æŸ¥æ˜¯å¦å·²ç»åˆå§‹åŒ–query_engine
    if 'query_engine' not in st.session_state:
        # init_models()
        st.session_state['query_engine'] = loadindex("test_storage").as_query_engine()

    # åˆå§‹åŒ–å¯¹è¯è®°å½•
    if "messages" not in st.session_state.keys():
        st.session_state.messages = [{"role": "assistant", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„åŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"}]

    # å±•ç¤ºèŠå¤©è®°å½•
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])

    # æ¸…ç©ºèŠå¤©è®°å½•
    st.sidebar.button('æ¸…é™¤å¯¹è¯è®°å½•', on_click=clear_chat_history)

    # åˆ·æ–°çŸ¥è¯†åº“
    if st.sidebar.button("åˆ·æ–°çŸ¥è¯†åº“"):
        st.session_state['query_engine'] = loadindex("test_storage").as_query_engine()
        st.success("çŸ¥è¯†åº“å·²åˆ·æ–°ï¼")

    # è¾“å…¥å¯¹è¯
    if prompt := st.chat_input():
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)

    # ç”Ÿæˆå›å¤
    if st.session_state.messages[-1]["role"] != "assistant":
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                response = generate_llama_index_response(prompt)
                placeholder = st.empty()
                placeholder.markdown(response)
        message = {"role": "assistant", "content": response}
        st.session_state.messages.append(message)
